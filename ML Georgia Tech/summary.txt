## input/output vector a probabilistic density function
## mutual information? (are these vectors similar?)
## Entropy? (does this feature have any info?)

## If the information is predictable or has less uncertainty, then it has less information.

## how many yes/nos == how many bits
~~ represented as a tree!
~~ the upper branches are those which have more information. (有针对性地问问题。)
~~ less question == less information (variable length encoding)

## one variable may contain information to infer the other.

## two variables...
~~ joint entropy, conditional entropy, mutualmutual information
~~ mutualmutual information measures "the degree to which one variable depends up one the other.."
  ~~ independent ... 0... no need to talk about information  (random?)
  ~~ dependent   ... >0... some information

## KL divergence -- measure of distance..
~~ distance of two distributions..
~~ in supervised learning, we have always some underlying distribution.
~~ it is another least square..


